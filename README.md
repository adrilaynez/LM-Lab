# ğŸ§ª LM Lab: Interactive Mechanics of Language Models

**LM Lab** is an educational observability suite designed to visualize the mathematical evolution of Large Language Models. 

Unlike standard "AI Wrappers," this project implements the entire history of Sequence Modelingâ€”from simple Bigram statistics to the complex self-attention mechanisms of modern Transformersâ€”**from scratch in PyTorch**.

The goal is not just to generate text, but to create a "Glass Box" where we can inspect:
* **Attention Maps:** How tokens "look" at each other.
* **Logit Lens:** How probability distributions evolve layer-by-layer.
* **Hidden States:** The geometry of the residual stream.

## ğŸš€ Roadmap & Architecture

| Level | Model | Concept Visualized | Status |
| :--- | :--- | :--- | :--- |
| **0** | **Bigram** | Statistical Co-occurrence | âœ… Done |
| **1** | **MLP** | Fixed Context Window (Bengio 2003) | ğŸš§ Planned |
| **2** | **RNN/GRU** | Recurrence & Vanishing Gradients | ğŸš§ Planned |
| **3** | **Transformer** | Self-Attention & Positional Encodings | ğŸš§ Planned |

## ğŸ› ï¸ Tech Stack
* **Core:** Python 3.10+, PyTorch `nn.Module`
* **Visualization:** Streamlit, Plotly
* **Observability:** Custom hooks for internal state extraction.

## ğŸ“¦ Usage
```bash
pip install -r requirements.txt
streamlit run app.py
